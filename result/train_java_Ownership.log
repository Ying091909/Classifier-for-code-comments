02/12/2023 02:25:53 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=10, config_name='', dev_filename='final_final_dataset/java/val_data_of_Ownership.jsonl', do_eval=True, do_lower_case=False, do_test=True, do_train=True, eval_batch_size=8, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path=None, local_rank=1, max_grad_norm=1.0, max_source_length=256, max_steps=-1, max_target_length=32, model_name_or_path=None, model_type=None, no_cuda=False, num_train_epochs=120, output_dir='java_Ownership_output', seed=42, test_filename='final_final_dataset/java/test_data_of_Ownership.jsonl', tokenizer_name='', train_batch_size=8, train_filename='final_final_dataset/java/train_data_of_Ownership.jsonl', train_log_filename='java_Ownership', train_steps=-1, visible_gpu='0, 1', warmup_steps=0, weight_decay=0.0)
02/12/2023 02:25:53 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
02/12/2023 02:25:53 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
02/12/2023 02:25:53 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True
02/12/2023 02:25:57 - INFO - __main__ -   model loaded!
02/12/2023 02:25:57 - INFO - __main__ -   *** Example ***
02/12/2023 02:25:57 - INFO - __main__ -   idx: 0
02/12/2023 02:25:57 - INFO - __main__ -   source_tokens: ['<s>', 'Classification', ':', '_@', 'author', '_va', 'adin', '_l', 'td', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   source_ids: 1 19168 30 632 4161 14162 25422 328 4465 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_tokens: ['<s>', 'true', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   target_ids: 1 3767 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   *** Example ***
02/12/2023 02:25:57 - INFO - __main__ -   idx: 1
02/12/2023 02:25:57 - INFO - __main__ -   source_tokens: ['<s>', 'Classification', ':', '_@', 'author', '_chr', 'is', '_p', 'ov', 'ir', 'k', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   source_ids: 1 19168 30 632 4161 4513 291 293 1527 481 79 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_tokens: ['<s>', 'true', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   target_ids: 1 3767 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   *** Example ***
02/12/2023 02:25:57 - INFO - __main__ -   idx: 2
02/12/2023 02:25:57 - INFO - __main__ -   source_tokens: ['<s>', 'Classification', ':', '_@', 'author', '_ke', 'vin', '_b', 'our', 'r', 'ill', 'ion', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   source_ids: 1 19168 30 632 4161 12519 21529 324 477 86 737 285 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_tokens: ['<s>', 'true', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   target_ids: 1 3767 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   *** Example ***
02/12/2023 02:25:57 - INFO - __main__ -   idx: 3
02/12/2023 02:25:57 - INFO - __main__ -   source_tokens: ['<s>', 'Classification', ':', '_@', 'author', '_va', 'adin', '_l', 'td', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   source_ids: 1 19168 30 632 4161 14162 25422 328 4465 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_tokens: ['<s>', 'true', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   target_ids: 1 3767 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   *** Example ***
02/12/2023 02:25:57 - INFO - __main__ -   idx: 4
02/12/2023 02:25:57 - INFO - __main__ -   source_tokens: ['<s>', 'Classification', ':', '_@', 'author', '_lo', 'u', 'is', '_w', 'ass', 'erman', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   source_ids: 1 19168 30 632 4161 437 89 291 341 428 29650 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_tokens: ['<s>', 'true', '</s>']
02/12/2023 02:25:57 - INFO - __main__ -   target_ids: 1 3767 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:57 - INFO - __main__ -   target_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02/12/2023 02:25:58 - INFO - __main__ -   ***** Running training *****
02/12/2023 02:25:58 - INFO - __main__ -     Num examples = 1840
02/12/2023 02:25:58 - INFO - __main__ -     Batch size = 8
02/12/2023 02:25:58 - INFO - __main__ -     Num epoch = 120
02/12/2023 02:25:59 - INFO - torch.nn.parallel.distributed -   Reducer buckets have been rebuilt in this iteration.
02/12/2023 02:26:28 - INFO - __main__ -   Epoch 0, step 99, train loss 9.846
002/12/2023 02:26:32 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:26:32 - INFO - __main__ -     Num examples = 89
02/12/2023 02:26:32 - INFO - __main__ -     Batch size = 802/12/2023 02:26:33 - INFO - __main__ -     eval_ppl = 1.32978
02/12/2023 02:26:33 - INFO - __main__ -     global_step = 116
02/12/2023 02:26:33 - INFO - __main__ -     train_loss = 9.511
02/12/2023 02:26:33 - INFO - __main__ -     ********************
02/12/2023 02:26:34 - INFO - __main__ -     Best ppl:1.32978
02/12/2023 02:26:34 - INFO - __main__ -     ********************
02/12/2023 02:26:38 - INFO - __main__ -   Epoch 0, the accuracy is 0.0
02/12/2023 02:27:07 - INFO - __main__ -   Epoch 1, step 99, train loss 0.6481
02/12/2023 02:27:12 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:27:12 - INFO - __main__ -     Num examples = 89
02/12/2023 02:27:12 - INFO - __main__ -     Batch size = 8
02/12/2023 02:27:12 - INFO - __main__ -     eval_ppl = 1.00172
02/12/2023 02:27:12 - INFO - __main__ -     global_step = 231
02/12/2023 02:27:12 - INFO - __main__ -     train_loss = 0.5698
02/12/2023 02:27:12 - INFO - __main__ -     ********************
02/12/2023 02:27:14 - INFO - __main__ -     Best ppl:1.00172
02/12/2023 02:27:14 - INFO - __main__ -     ********************
0202/12/2023 02:27:16 - INFO - __main__ -   Epoch 1, the accuracy is 0.95505617977528002/12/2023 02:27:45 - INFO - __main__ -   Epoch 2, step 99, train loss 0.049
02/12/2023 02:27:50 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:27:50 - INFO - __main__ -     Num examples = 89
02/12/2023 02:27:50 - INFO - __main__ -     Batch size = 8
02/12/2023 02:27:50 - INFO - __main__ -     eval_ppl = 1.00043
02/12/2023 02:27:50 - INFO - __main__ -     global_step = 346
02/12/2023 02:27:50 - INFO - __main__ -     train_loss = 0.0475
02/12/2023 02:27:50 - INFO - __main__ -     ********************
02/12/2023 02:27:52 - INFO - __main__ -     Best ppl:1.00043
02/12/2023 02:27:52 - INFO - __main__ -     ********************
0202/12/2023 02:27:54 - INFO - __main__ -   Epoch 2, the accuracy is 0.9887640449438200202/12/2023 02:28:23 - INFO - __main__ -   Epoch 3, step 99, train loss 0.01202/12/2023 02:28:28 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:28:28 - INFO - __main__ -     Num examples = 89
02/12/2023 02:28:28 - INFO - __main__ -     Batch size = 8
02/12/2023 02:28:28 - INFO - __main__ -     eval_ppl = 1.00001
02/12/2023 02:28:28 - INFO - __main__ -     global_step = 461
02/12/2023 02:28:28 - INFO - __main__ -     train_loss = 0.0117
02/12/2023 02:28:28 - INFO - __main__ -     ********************
0202/12/2023 02:28:30 - INFO - __main__ -     Best ppl:1.00001
02/12/2023 02:28:30 - INFO - __main__ -     *******************02/12/2023 02:28:32 - INFO - __main__ -   Epoch 3, the accuracy is 1.0
0202/12/2023 02:29:01 - INFO - __main__ -   Epoch 4, step 99, train loss 0.0050202/12/2023 02:29:06 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:29:06 - INFO - __main__ -     Num examples = 89
02/12/2023 02:29:06 - INFO - __main__ -     Batch size = 02/12/2023 02:29:06 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:29:06 - INFO - __main__ -     global_step = 576
02/12/2023 02:29:06 - INFO - __main__ -     train_loss = 0.0027
02/12/2023 02:29:06 - INFO - __main__ -     ********************
02/12/2023 02:29:08 - INFO - __main__ -     Best ppl:1.0
02/12/2023 02:29:08 - INFO - __main__ -     ********************
0202/12/2023 02:29:10 - INFO - __main__ -   Epoch 4, the accuracy is 1.02/12/2023 02:29:39 - INFO - __main__ -   Epoch 5, step 99, train loss 0.0028
02/12/2023 02:29:44 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:29:44 - INFO - __main__ -     Num examples = 89
02/12/2023 02:29:44 - INFO - __main__ -     Batch size = 8
02/12/2023 02:29:44 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:29:44 - INFO - __main__ -     global_step = 691
02/12/2023 02:29:44 - INFO - __main__ -     train_loss = 0.0026
02/12/2023 02:29:44 - INFO - __main__ -     ********************
02/12/2023 02:29:46 - INFO - __main__ -     Best ppl:1.0
02/12/2023 02:29:46 - INFO - __main__ -     ********************
0202/12/2023 02:29:48 - INFO - __main__ -   Epoch 5, the accuracy is 1.0202/12/2023 02:30:17 - INFO - __main__ -   Epoch 6, step 99, train loss 0.00402/12/2023 02:30:22 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:30:22 - INFO - __main__ -     Num examples = 89
02/12/2023 02:30:22 - INFO - __main__ -     Batch size = 8
02/12/2023 02:30:22 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:30:22 - INFO - __main__ -     global_step = 806
02/12/2023 02:30:22 - INFO - __main__ -     train_loss = 0.0003
02/12/2023 02:30:22 - INFO - __main__ -     ********************
02/12/2023 02:30:24 - INFO - __main__ -     Best ppl:1.0
02/12/2023 02:30:24 - INFO - __main__ -     ********************
0202/12/2023 02:30:26 - INFO - __main__ -   Epoch 6, the accuracy is 1.02/12/2023 02:30:55 - INFO - __main__ -   Epoch 7, step 99, train loss 0.0006
0202/12/2023 02:31:00 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:31:00 - INFO - __main__ -     Num examples = 89
02/12/2023 02:31:00 - INFO - __main__ -     Batch size = 02/12/2023 02:31:00 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:31:00 - INFO - __main__ -     global_step = 921
02/12/2023 02:31:00 - INFO - __main__ -     train_loss = 0.0006
02/12/2023 02:31:00 - INFO - __main__ -     ********************
02/12/2023 02:31:03 - INFO - __main__ -   Epoch 7, the accuracy is 1.0
02/12/2023 02:31:31 - INFO - __main__ -   Epoch 8, step 99, train loss 0.0002
02/12/2023 02:31:36 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:31:36 - INFO - __main__ -     Num examples = 89
02/12/2023 02:31:36 - INFO - __main__ -     Batch size = 8
02/12/2023 02:31:37 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:31:37 - INFO - __main__ -     global_step = 1036
02/12/2023 02:31:37 - INFO - __main__ -     train_loss = 0.0003
02/12/2023 02:31:37 - INFO - __main__ -     ********************
02/12/2023 02:31:39 - INFO - __main__ -   Epoch 8, the accuracy is 1.0
02/12/2023 02:32:08 - INFO - __main__ -   Epoch 9, step 99, train loss 0.0003
02/12/2023 02:32:13 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:32:13 - INFO - __main__ -     Num examples = 89
02/12/2023 02:32:13 - INFO - __main__ -     Batch size = 8
02/12/2023 02:32:14 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:32:14 - INFO - __main__ -     global_step = 1151
02/12/2023 02:32:14 - INFO - __main__ -     train_loss = 0.0003
02/12/2023 02:32:14 - INFO - __main__ -     ********************
02/12/2023 02:32:16 - INFO - __main__ -   Epoch 9, the accuracy is 1.0
02/12/2023 02:32:45 - INFO - __main__ -   Epoch 10, step 99, train loss 0.0003
02/12/2023 02:32:50 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:32:50 - INFO - __main__ -     Num examples = 89
02/12/2023 02:32:50 - INFO - __main__ -     Batch size = 8
02/12/2023 02:32:51 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:32:51 - INFO - __main__ -     global_step = 1266
02/12/2023 02:32:51 - INFO - __main__ -     train_loss = 0.0003
02/12/2023 02:32:51 - INFO - __main__ -     ********************
02/12/2023 02:32:53 - INFO - __main__ -   Epoch 10, the accuracy is 1.0
02/12/2023 02:33:22 - INFO - __main__ -   Epoch 11, step 99, train loss 0.0002
02/12/2023 02:33:27 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:33:27 - INFO - __main__ -     Num examples = 89
02/12/2023 02:33:27 - INFO - __main__ -     Batch size = 8
02/12/2023 02:33:28 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:33:28 - INFO - __main__ -     global_step = 1381
02/12/2023 02:33:28 - INFO - __main__ -     train_loss = 0.0002
02/12/2023 02:33:28 - INFO - __main__ -     ********************
02/12/2023 02:33:30 - INFO - __main__ -   Epoch 11, the accuracy is 1.0
02/12/2023 02:33:59 - INFO - __main__ -   Epoch 12, step 99, train loss 0.0003
02/12/2023 02:34:03 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:34:03 - INFO - __main__ -     Num examples = 89
02/12/2023 02:34:03 - INFO - __main__ -     Batch size = 8
02/12/2023 02:34:04 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:34:04 - INFO - __main__ -     global_step = 1496
02/12/2023 02:34:04 - INFO - __main__ -     train_loss = 0.0003
02/12/2023 02:34:04 - INFO - __main__ -     ********************
02/12/2023 02:34:07 - INFO - __main__ -   Epoch 12, the accuracy is 1.0
02/12/2023 02:34:36 - INFO - __main__ -   Epoch 13, step 99, train loss 0.0007
02/12/2023 02:34:40 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:34:40 - INFO - __main__ -     Num examples = 89
02/12/2023 02:34:40 - INFO - __main__ -     Batch size = 8
02/12/2023 02:34:41 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:34:41 - INFO - __main__ -     global_step = 1611
02/12/2023 02:34:41 - INFO - __main__ -     train_loss = 0.0006
02/12/2023 02:34:41 - INFO - __main__ -     ********************
02/12/2023 02:34:42 - INFO - __main__ -     Best ppl:1.0
02/12/2023 02:34:42 - INFO - __main__ -     ********************
02/12/2023 02:34:45 - INFO - __main__ -   Epoch 13, the accuracy is 1.0
02/12/2023 02:35:14 - INFO - __main__ -   Epoch 14, step 99, train loss 0.0002
02/12/2023 02:35:19 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:35:19 - INFO - __main__ -     Num examples = 89
02/12/2023 02:35:19 - INFO - __main__ -     Batch size = 8
02/12/2023 02:35:20 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:35:20 - INFO - __main__ -     global_step = 1726
02/12/2023 02:35:20 - INFO - __main__ -     train_loss = 0.0002
02/12/2023 02:35:20 - INFO - __main__ -     ********************
02/12/2023 02:35:22 - INFO - __main__ -   Epoch 14, the accuracy is 1.0
02/12/2023 02:35:51 - INFO - __main__ -   Epoch 15, step 99, train loss 0.0002
02/12/2023 02:35:56 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:35:56 - INFO - __main__ -     Num examples = 89
02/12/2023 02:35:56 - INFO - __main__ -     Batch size = 8
02/12/2023 02:35:57 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:35:57 - INFO - __main__ -     global_step = 1841
02/12/2023 02:35:57 - INFO - __main__ -     train_loss = 0.0002
02/12/2023 02:35:57 - INFO - __main__ -     ********************
02/12/2023 02:35:59 - INFO - __main__ -   Epoch 15, the accuracy is 1.0
02/12/2023 02:36:28 - INFO - __main__ -   Epoch 16, step 99, train loss 0.0001
02/12/2023 02:36:33 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:36:33 - INFO - __main__ -     Num examples = 89
02/12/2023 02:36:33 - INFO - __main__ -     Batch size = 8
02/12/2023 02:36:33 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:36:33 - INFO - __main__ -     global_step = 1956
02/12/2023 02:36:33 - INFO - __main__ -     train_loss = 0.0001
02/12/2023 02:36:33 - INFO - __main__ -     ********************
02/12/2023 02:36:36 - INFO - __main__ -   Epoch 16, the accuracy is 1.0
02/12/2023 02:37:05 - INFO - __main__ -   Epoch 17, step 99, train loss 0.0006
02/12/2023 02:37:09 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:37:09 - INFO - __main__ -     Num examples = 89
02/12/2023 02:37:09 - INFO - __main__ -     Batch size = 8
02/12/2023 02:37:10 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:37:10 - INFO - __main__ -     global_step = 2071
02/12/2023 02:37:10 - INFO - __main__ -     train_loss = 0.0005
02/12/2023 02:37:10 - INFO - __main__ -     ********************
002/12/2023 02:37:12 - INFO - __main__ -   Epoch 17, the accuracy is 1.0002/12/2023 02:37:41 - INFO - __main__ -   Epoch 18, step 99, train loss 0.002/12/2023 02:37:46 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:37:46 - INFO - __main__ -     Num examples = 89
02/12/2023 02:37:46 - INFO - __main__ -     Batch size = 8
02/12/2023 02:37:46 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:37:46 - INFO - __main__ -     global_step = 2186
02/12/2023 02:37:46 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:37:46 - INFO - __main__ -     ********************
02/12/02/12/2023 02:37:49 - INFO - __main__ -   Epoch 18, the accuracy i02/12/02/12/2023 02:38:17 - INFO - __main__ -   Epoch 19, step 99, train loss 002/12/2023 02:38:22 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:38:22 - INFO - __main__ -     Num examples = 89
02/12/2023 02:38:22 - INFO - __main__ -     Batch size = 8
02/12/2023 02:38:23 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:38:23 - INFO - __main__ -     global_step = 2301
02/12/2023 02:38:23 - INFO - __main__ -     train_loss = 0.0045
02/12/2023 02:38:23 - INFO - __main__ -     ********************
02/12/2023 02:38:25 - INFO - __main__ -   Epoch 19, the accuracy is 1.0
02/12/2023 02:38:54 - INFO - __main__ -   Epoch 20, step 99, train loss 0.0001
02/12/2023 02:38:59 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:38:59 - INFO - __main__ -     Num examples = 89
02/12/2023 02:38:59 - INFO - __main__ -     Batch size = 8
02/12/2023 02:39:00 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:39:00 - INFO - __main__ -     global_step = 2416
02/12/2023 02:39:00 - INFO - __main__ -     train_loss = 0.0001
02/12/2023 02:39:00 - INFO - __main__ -     ********************
02/12/2023 02:39:02 - INFO - __main__ -   Epoch 20, the accuracy is 1.0
02/12/02/12/2023 02:39:31 - INFO - __main__ -   Epoch 21, step 99, train loss 002/12/2023 02:39:36 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:39:36 - INFO - __main__ -     Num examples = 89
02/12/2023 02:39:36 - INFO - __main__ -     Batch size = 8
02/12/2023 02:39:36 - INFO - __main__ -     eval_ppl = 1.00001
02/12/2023 02:39:36 - INFO - __main__ -     global_step = 2531
02/12/2023 02:39:36 - INFO - __main__ -     train_loss = 0.0002
02/12/2023 02:39:36 - INFO - __main__ -     ********************
02/12/02/12/2023 02:39:39 - INFO - __main__ -   Epoch 21, the accuracy i02/12/2023 02:40:07 - INFO - __main__ -   Epoch 22, step 99, train loss 0.0001
02/12/2023 02:40:12 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:40:12 - INFO - __main__ -     Num examples = 89
02/12/2023 02:40:12 - INFO - __main__ -     Batch size = 8
02/12/2023 02:40:13 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:40:13 - INFO - __main__ -     global_step = 2646
02/12/2023 02:40:13 - INFO - __main__ -     train_loss = 0.0001
02/12/2023 02:40:13 - INFO - __main__ -     ********************
02/12/2023 02:40:15 - INFO - __main__ -   Epoch 22, the accuracy is 1.0
02/12/02/12/2023 02:40:44 - INFO - __main__ -   Epoch 23, step 99, train loss 002/12/2023 02:40:48 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:40:48 - INFO - __main__ -     Num examples = 89
02/12/2023 02:40:48 - INFO - __main__ -     Batch size = 8
02/12/2023 02:40:49 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:40:49 - INFO - __main__ -     global_step = 2761
02/12/2023 02:40:49 - INFO - __main__ -     train_loss = 0.0001
02/12/2023 02:40:49 - INFO - __main__ -     ********************
02/12/02/12/2023 02:40:51 - INFO - __main__ -   Epoch 23, the accuracy i02/12/2023 02:41:20 - INFO - __main__ -   Epoch 24, step 99, train loss 0.0002
02/12/2023 02:41:25 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:41:25 - INFO - __main__ -     Num examples = 89
02/12/2023 02:41:25 - INFO - __main__ -     Batch size = 8
02/12/2023 02:41:25 - INFO - __main__ -     eval_ppl = 1.00001
02/12/2023 02:41:25 - INFO - __main__ -     global_step = 2876
02/12/2023 02:41:25 - INFO - __main__ -     train_loss = 0.0001
02/12/2023 02:41:25 - INFO - __main__ -     ********************
02/12/02/12/2023 02:41:28 - INFO - __main__ -   Epoch 24, the accuracy i02/12/2023 02:41:56 - INFO - __main__ -   Epoch 25, step 99, train loss 0.0001
02/12/2023 02:42:01 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:42:01 - INFO - __main__ -     Num examples = 89
02/12/2023 02:42:01 - INFO - __main__ -     Batch size = 8
02/12/2023 02:42:02 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:42:02 - INFO - __main__ -     global_step = 2991
02/12/2023 02:42:02 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:42:02 - INFO - __main__ -     ********************
02/12/2002/12/2023 02:42:04 - INFO - __main__ -   Epoch 25, the accuracy02/12/2002/12/2023 02:42:33 - INFO - __main__ -   Epoch 26, step 99, train l02/12/2023 02:42:37 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:42:37 - INFO - __main__ -     Num examples = 89
02/12/2023 02:42:37 - INFO - __main__ -     Batch size = 8
02/12/2023 02:42:38 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:42:38 - INFO - __main__ -     global_step = 3106
02/12/2023 02:42:38 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:42:38 - INFO - __main__ -     ********************
02/12/2023 0202/12/2023 02:42:41 - INFO - __main__ -   Epoch 26, the acc02/12/2023 0202/12/2023 02:43:09 - INFO - __main__ -   Epoch 27, step 99, tr02/12/2023 02:43:14 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:43:14 - INFO - __main__ -     Num examples = 89
02/12/2023 02:43:14 - INFO - __main__ -     Batch size = 8
02/12/2023 02:43:15 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:43:15 - INFO - __main__ -     global_step = 3221
02/12/2023 02:43:15 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:43:15 - INFO - __main__ -     ********************
02/12/2023 02:43:1702/12/2023 02:43:17 - INFO - __main__ -   Epoch 27, t02/12/2023 02:43:4502/12/2023 02:43:45 - INFO - __main__ -   Epoch 28, step 02/12/2023 02:43:50 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:43:50 - INFO - __main__ -     Num examples = 89
02/12/2023 02:43:50 - INFO - __main__ -     Batch size = 8
02/12/2023 02:43:51 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:43:51 - INFO - __main__ -     global_step = 3336
02/12/2023 02:43:51 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:43:51 - INFO - __main__ -     ********************
02/12/2023 02:43:52 - INFO - __main__ -     Best ppl:1.0
02/12/2023 02:43:52 - INFO - __main__ -     ********************
02/12/2023 02:43:55 - INF02/12/2023 02:43:55 - INFO - __main__ -   Epoch02/12/2023 02:44:23 - INF02/12/2023 02:44:23 - INFO - __main__ -   Epoch 29,02/12/2023 02:44:28 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:44:28 - INFO - __main__ -     Num examples = 89
02/12/2023 02:44:28 - INFO - __main__ -     Batch size = 8
02/12/2023 02:44:29 - INFO - __main__ -     eval_ppl = 1.00001
02/12/2023 02:44:29 - INFO - __main__ -     global_step = 3451
02/12/2023 02:44:29 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:44:29 - INFO - __main__ -     ********************
02/12/2023 02:44:31 - INFO - __02/12/2023 02:44:31 - INFO - __main__ -  02/12/2023 02:45:00 - INFO - __main__ -   Epoch 30, step 99, train loss 0.0001
02/12/2023 02:45:04 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:45:04 - INFO - __main__ -     Num examples = 89
02/12/2023 02:45:04 - INFO - __main__ -     Batch size = 8
02/12/2023 02:45:05 - INFO - __main__ -     eval_ppl = 1.00001
02/12/2023 02:45:05 - INFO - __main__ -     global_step = 3566
02/12/2023 02:45:05 - INFO - __main__ -     train_loss = 0.0001
02/12/2023 02:45:05 - INFO - __main__ -     ********************
02/12/2023 02:45:08 - INFO - __main__ -   Epoch 30, the accuracy is 1.0
02/12/2023 02:45:36 - INFO - __main__ -   Epoch 31, step 99, train loss 0.0
02/12/2023 02:45:41 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:45:41 - INFO - __main__ -     Num examples = 89
02/12/2023 02:45:41 - INFO - __main__ -     Batch size = 8
02/12/2023 02:45:42 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:45:42 - INFO - __main__ -     global_step = 3681
02/12/2023 02:45:42 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:45:42 - INFO - __main__ -     ********************
02/12/2023 02:45:44 - INFO - __main__ -   Epoch 31, the accuracy is 1.0
02/12/2023 02:46:13 - INFO - __main_02/12/2023 02:46:13 - INFO - __main__ - 02/12/2023 02:46:17 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:46:17 - INFO - __main__ -     Num examples = 89
02/12/2023 02:46:17 - INFO - __main__ -     Batch size = 8
02/12/2023 02:46:18 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:46:18 - INFO - __main__ -     global_step = 3796
02/12/2023 02:46:18 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:46:18 - INFO - __main__ -     ********************
02/12/2023 02:46:20 - INFO - __main__ -   Epoch 32, the accuracy is 1.0
02/12/2023 02:46:49 - INFO - __main__ -   Epoch 33, step 99, train loss 0.0
02/12/2023 02:46:54 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:46:54 - INFO - __main__ -     Num examples = 89
02/12/2023 02:46:54 - INFO - __main__ -     Batch size = 8
02/12/2023 02:46:54 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:46:54 - INFO - __main__ -     global_step = 3911
02/12/2023 02:46:54 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:46:54 - INFO - __main__ -     ********************
02/12/2023 02:46:57 - INFO - __main__ -   Epoch 33, the accuracy is 1.0
02/12/2023 02:47:25 - INFO - __main__ -   Epoch 34, step 99, train loss 0.0
02/12/2023 02:47:30 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:47:30 - INFO - __main__ -     Num examples = 89
02/12/2023 02:47:30 - INFO - __main__ -     Batch size = 8
02/12/2023 02:47:31 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:47:31 - INFO - __main__ -     global_step = 4026
02/12/2023 02:47:31 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:47:31 - INFO - __main__ -     ********************
02/12/2023 02:47:33 - INFO - __main__ -   Epoch 34, the accuracy is 1.0
02/12/2023 02:48:02 - INFO - __main__ -   Epoch 35, st02/12/2023 02:48:02 - 02/12/2023 02:48:07 - INFO - __main__ -   
***** Running 02/12/2023 02:48:07 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:48:07 - INFO - __main__ -     Num examples = 89
0202/12/2023 02:48:07 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:48:07 - INFO - __main__ -     global_step = 4141
02/12/2023 02:48:07 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:48:07 - INFO - __main__ -     ********************
02/12/2023 02:48:10 - INFO - __main__ -   Epoch 35, the accuracy is 1.0
02/12/2023 02:48:38 - INFO - __main__ -   Epoch 36, step 99, train loss 0.0
02/12/2023 02:48:43 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:48:43 - INFO - __main__ -     Num examples = 89
02/12/2023 02:48:43 - INFO - __main__ -     Batch size = 8
02/12/2023 02:48:44 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:48:44 - INFO - __main__ -     global_step = 4256
02/12/2023 02:48:44 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:48:44 - INFO - __main__ -     ********************
02/12/2023 02:48:46 - INFO - __main__ -   Epoch 36, the accuracy i02/12/02/12/2023 02:49:15 - INFO - __main__ -   Epoch 37, step 99, train loss 0.0
02/12/2023 02:49:19 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:49:19 - INFO - __main__ -     Num examples = 89
02/12/2023 02:49:19 - INFO - __main__ -     Batch size = 8
02/12/2023 02:49:20 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:49:20 - INFO - __main__ -     global_step = 4371
02/12/2023 02:49:20 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:49:20 - INFO - __main__ -     ********************
02/12/2023 02:49:23 - INFO - __main__ -   Epoch 37, the accuracy is 1.0
02/12/2023 02:49:51 - INFO - __main__ -   Epoch 38, step 99, train loss 0.0
02/12/2023 02:49:56 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:49:56 - INFO - __main__ -     Num examples = 89
02/12/2023 02:49:56 - INFO - __main__ -     Batch size = 8
02/12/2023 02:49:57 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:49:57 - INFO - __main__ -     global_step = 4486
02/12/2023 02:49:57 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:49:57 - INFO - __main__ -     ********************
02/12/2023 02:49:59 - INFO - __main__ -   Epoch 38, the accuracy is 1.0
02/12/2023 02:50:28 - INFO - __main__ -   Epoch 39, step 99, train loss 0.0
02/12/2023 02:50:32 - INFO - __main__ -   
***** Running evaluation *****
02/12/02/12/2023 02:50:32 - INFO - __main__ -   
***** Running02/12/2023 02:50:32 - INFO - __main__ -     Batch size = 8
02/12/2023 02:50:33 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:50:33 - INFO - __main__ -     global_step = 4601
02/12/2023 02:50:33 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:50:33 - INFO - __main__ -     ********************
02/12/2023 02:50:36 - INFO - __main__ -   Epoch 39, the accuracy is 1.0
02/12/2023 02:51:04 - INFO - __main__ -   Epoch 40, step 99, train loss 0.0
02/12/2023 02:51:09 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:51:09 - INFO - __main__ -     Num examples = 89
02/12/2023 02:51:09 - INFO - __main__ -     Batch size = 8
02/12/2023 02:51:10 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:51:10 - INFO - __main__ -     global_step = 4716
02/12/2023 02:51:10 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:51:10 - INFO - __main__ -     ********************
02/12/2023 02:51:11 - INFO - __main__ -     Best ppl:1.0
02/12/2023 02:51:11 - INFO - __main__ -     ********************
02/12/2023 02:51:14 - INFO - __main__ -   Epoch 40, the accuracy is 1.0
02/12/2023 02:51:42 - INFO - __main__ -   Epoch 41, step 99, train loss 0.0
02/12/2023 02:51:47 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:51:47 - INFO - __main__ -     Num examples = 89
02/12/2023 02:51:47 - INFO - __main__ -     Batch size = 8
02/12/2023 02:51:48 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:51:48 - INFO - __main__ -     global_step = 4831
02/12/2023 02:51:48 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:51:48 - INFO - __main__ -     ********************
02/12/2023 02:51:50 - INFO - __main__ -   Epoch 41, the accuracy is 1.0
02/12/2023 02:52:19 - INFO - __main__ -   Epoch 42, step 99, train loss 0.0
02/12/2023 02:52:24 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:52:24 - INFO - __main__ -     Num examples = 89
02/12/2023 02:52:24 - INFO - __main__ -     Batch size = 8
02/12/2023 02:52:25 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:52:25 - INFO - __main__ -     global_step = 4946
02/12/2023 02:52:25 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:52:25 - INFO - __main__ -     ********************
02/12/2023 02:52:27 - INFO - __main__ -   Epoch 42, the accuracy is 1.0
02/12/2023 02:52:56 - INFO - __main__ -   Epoch 43, step 99, train loss 0.0
02/12/2023 02:53:01 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:53:01 - INFO - __main__ -     Num examples = 89
02/12/2023 02:53:01 - INFO - __main__ -     Batch size = 8
02/12/2023 02:53:02 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:53:02 - INFO - __main__ -     global_step = 5061
02/12/2023 02:53:02 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:53:02 - INFO - __main__ -     ********************
02/12/2023 02:53:03 - INFO - __main__ -     Best ppl:1.0
02/12/2023 02:53:03 - INFO - __main__ -     ********************
02/12/2023 02:53:05 - INFO - __main__ -   Epoch 43, the accuracy is 1.0
02/12/2023 02:53:34 - INFO - __main__ -   Epoch 44, step 99, train loss 0.0
02/12/2023 02:53:39 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:53:39 - INFO - __main__ -     Num examples = 89
02/12/2023 02:53:39 - INFO - __main__ -     Batch size = 8
02/12/2023 02:53:39 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:53:39 - INFO - __main__ -     global_step = 5176
02/12/2023 02:53:39 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:53:39 - INFO - __main__ -     ********************
02/12/2023 02:53:42 - INFO - __main__ -   Epoch 44, the accuracy is 1.0
02/12/2023 02:54:10 - INFO - __main__ -   Epoch 45, step 99, train loss 0.0
02/12/2023 02:54:15 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:54:15 - INFO - __main__ -     Num examples = 89
02/12/2023 02:54:15 - INFO - __main__ -     Batch size = 8
02/12/2023 02:54:16 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:54:16 - INFO - __main__ -     global_step = 5291
02/12/2023 02:54:16 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:54:16 - INFO - __main__ -     ********************
02/12/2023 02:54:18 - INFO - __main__ -   Epoch 45, the accuracy is 1.0
02/12/2023 02:54:47 - INFO - __main__ -   Epoch 46, step 99, train loss 0.0
02/12/2023 02:54:51 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:54:51 - INFO - __main__ -     Num examples = 89
02/12/2023 02:54:51 - INFO - __main__ -     Batch size = 8
02/12/2023 02:54:52 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:54:52 - INFO - __main__ -     global_step = 5406
02/12/2023 02:54:52 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:54:52 - INFO - __main__ -     ********************
02/12/2023 02:54:55 - INFO - __main__ -   Epoch 46, the accuracy is 1.0
02/12/2023 02:55:23 - INFO - __main__ -   Epoch 47, step 99, train loss 0.0
02/12/2023 02:55:28 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:55:28 - INFO - __main__ -     Num examples = 89
02/12/2023 02:55:28 - INFO - __main__ -     Batch size = 8
02/12/2023 02:55:29 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:55:29 - INFO - __main__ -     global_step = 5521
02/12/2023 02:55:29 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:55:29 - INFO - __main__ -     ********************
02/12/2023 02:55:31 - INFO - __main__ -   Epoch 47, the accuracy is 1.0
02/12/2023 02:56:00 - INFO - __main__ -   Epoch 48, step 99, train loss 0.0
02/12/2023 02:56:04 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:56:04 - INFO - __main__ -     Num examples = 02/02/12/2023 02:56:04 - INFO - __main__ -     Batch size = 8
02/12/2023 02:56:05 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:56:05 - INFO - __main__ -     global_step = 5636
02/12/2023 02:56:05 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:56:05 - INFO - __main__ -     ********************
02/12/2023 02:56:08 - INFO - __main__ -   Epoch 48, the accuracy is 1.0
02/12/2023 02:56:37 - INFO - __main__ -   Epoch 49, step 99, train loss 0.0
02/12/2023 02:56:41 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:56:41 - INFO - __main__ -     Num examples = 89
02/12/2023 02:56:41 - INFO - __main__ -     Batch size = 8
02/12/2023 02:56:42 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:56:42 - INFO - __main__ -     global_step = 5751
02/12/2023 02:56:42 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:56:42 - INFO - __main__ -     ********************
02/12/2023 02:56:45 - INFO - __main__ -   Epoch 49, the accuracy is 1.0
02/12/2023 02:57:13 - INFO - __main__ -   Epoch 50, step 99, train loss 0.0
02/12/2023 02:57:18 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:57:18 - INFO - __main__ -     Num examples = 89
02/12/2023 02:57:18 - INFO - __main__ -     Batch size = 8
02/12/2023 02:57:19 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:57:19 - INFO - __main__ -     global_step = 5866
02/12/2023 02:57:19 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:57:19 - INFO - __main__ -     ********************
02/12/2023 02:57:21 - INFO - __main__ -   Epoch 50, the accuracy is 1.0
02/12/2023 02:57:50 - INFO - __main__ -   Epoch 51, step 99, train loss 0.0
02/12/2023 02:57:54 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:57:54 - INFO - __main__ -     Num examples = 89
02/12/2023 02:57:54 - INFO - __main__ -     Batch size = 8
02/12/2023 02:57:55 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:57:55 - INFO - __main__ -     global_step = 5981
02/12/2023 02:57:55 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:57:55 - INFO - __main__ -     ********************
02/12/2023 02:57:58 - INFO - __main__ -   Epoch 51, the accuracy is 1.0
02/12/2023 02:58:26 - INFO - __main__ -   Epoch 52, step 99, train loss 0.0
02/12/2023 02:58:31 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:58:31 - INFO - __main__ -     Num examples = 89
02/12/2023 02:58:31 - INFO - __main__ -     Batch size = 8
02/12/2023 02:58:32 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:58:32 - INFO - __main__ -     global_step = 6096
02/12/2023 02:58:32 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:58:32 - INFO - __main__ -     ********************
02/12/2023 02:58:34 - INFO - __main__ -   Epoch 52, the accuracy is 1.0
02/12/2023 02:59:03 - INFO - __main__ -   Epoch 53, step 99, train loss 0.0
02/12/2023 02:59:07 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:59:07 - INFO - __main__ -     Num examples = 89
02/12/2023 02:59:07 - INFO - __main__ -     Batch size = 8
02/12/2023 02:59:08 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:59:08 - INFO - __main__ -     global_step = 6211
02/12/2023 02:59:08 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:59:08 - INFO - __main__ -     ********************
02/12/2023 02:59:10 - INFO - __main__ -   Epoch 53, the accuracy is 1.0
02/12/2023 02:59:39 - INFO - __main__ -   Epoch 54, step 99, train loss 0.0
02/12/2023 02:59:44 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 02:59:44 - INFO - __main__ -     Num examples = 89
02/12/2023 02:59:44 - INFO - __main__ -     Batch size = 8
02/12/2023 02:59:45 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 02:59:45 - INFO - __main__ -     global_step = 6326
02/12/2023 02:59:45 - INFO - __main__ -     train_loss = 0.0
02/12/2023 02:59:45 - INFO - __main__ -     ********************
02/12/2023 02:59:46 - INFO - __main__ -     Best ppl:1.0
02/12/2023 02:59:46 - INFO - __main__ -     ********************
02/12/2023 02:59:49 - INFO - __main__ -   Epoch 54, the accuracy is 1.0
02/12/2023 03:00:18 - INFO - __main__ -   Epoch 55, step 99, train loss 0.0
02/12/2023 03:00:22 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:00:22 - INFO - __main__ -     Num examples = 89
02/12/2023 03:00:22 - INFO - __main__ -     Batch size = 8
02/12/2023 03:00:23 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:00:23 - INFO - __main__ -     global_step = 6441
02/12/2023 03:00:23 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:00:23 - INFO - __main__ -     ********************
02/12/2023 03:00:26 - INFO - __main__ -   Epoch 55, the accuracy is 1.0
02/12/2023 03:00:55 - INFO - __main__ -   Epoch 56, step 99, train loss 0.0
02/12/2023 03:00:59 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:00:59 - INFO - __main__ -     Num examples = 89
02/12/2023 03:00:59 - INFO - __main__ -     Batch size = 8
02/12/2023 03:01:00 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:01:00 - INFO - __main__ -     global_step = 6556
02/12/2023 03:01:00 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:01:00 - INFO - __main__ -     ********************
02/12/2023 03:01:03 - INFO - __main__ -   Epoch 56, the accuracy is 1.0
02/12/2023 03:01:31 - INFO - __main__ -   Epoch 57, step 99, train loss 0.0
02/12/2023 03:01:36 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:01:36 - INFO - __main__ -     Num examples = 89
02/12/2023 03:01:36 - INFO - __main__ -     Batch size = 8
02/12/2023 03:01:37 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:01:37 - INFO - __main__ -     global_step = 6671
02/12/2023 03:01:37 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:01:37 - INFO - __main__ -     ********************
02/12/2023 03:01:38 - INFO - __main__ -     Best ppl:1.0
02/12/2023 03:01:38 - INFO - __main__ -     ********************
02/12/2023 03:01:41 - INFO - __main__ -   Epoch 57, the accuracy is 1.0
02/12/2023 03:02:09 - INFO - __main__ -   Epoch 58, step 99, train loss 0.0
02/12/2023 03:02:14 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:02:14 - INFO - __main__ -     Num examples = 89
02/12/2023 03:02:14 - INFO - __main__ -     Batch size = 8
02/12/2023 03:02:15 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:02:15 - INFO - __main__ -     global_step = 6786
02/12/2023 03:02:15 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:02:15 - INFO - __main__ -     ********************
02/12/2023 03:02:17 - INFO - __main__ -   Epoch 58, the accuracy is 1.0
02/12/2023 03:02:46 - INFO - __main__ -   Epoch 59, step 99, train loss 0.0
02/12/2023 03:02:50 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:02:50 - INFO - __main__ -     Num examples = 89
02/12/2023 03:02:50 - INFO - __main__ -     Batch size = 8
02/12/2023 03:02:51 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:02:51 - INFO - __main__ -     global_step = 6901
02/12/2023 03:02:51 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:02:51 - INFO - __main__ -     ********************
02/12/2023 03:02:54 - INFO - __main__ -   Epoch 59, the accuracy is 1.0
02/12/2023 03:03:23 - INFO - __main__ -   Epoch 60, step 99, train loss 0.0
02/12/2023 03:03:27 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:03:27 - INFO - __main__ -     Num examples = 89
02/12/2023 03:03:27 - INFO - __main__ -     Batch size = 8
02/12/2023 03:03:28 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:03:28 - INFO - __main__ -     global_step = 7016
02/12/2023 03:03:28 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:03:28 - INFO - __main__ -     ********************
02/12/2023 03:03:31 - INFO - __main__ -   Epoch 60, the accuracy is 1.0
02/12/2023 03:03:59 - INFO - __main__ -   Epoch 61, step 99, train loss 0.0
02/12/2023 03:04:04 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:04:04 - INFO - __main__ -     Num examples = 89
02/12/2023 03:04:04 - INFO - __main__ -     Batch size = 8
02/12/2023 03:04:05 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:04:05 - INFO - __main__ -     global_step = 7131
02/12/2023 03:04:05 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:04:05 - INFO - __main__ -     ********************
02/12/2023 03:04:07 - INFO - __main__ -   Epoch 61, the accuracy is 1.0
02/12/2023 03:04:36 - INFO - __main__ -   Epoch 62, step 99, train loss 0.0
02/12/2023 03:04:41 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:04:41 - INFO - __main__ -     Num examples = 89
02/12/2023 03:04:41 - INFO - __main__ -     Batch size = 8
02/12/2023 03:04:41 - INFO - __main__ -     eval_ppl = 1.00002
02/12/2023 03:04:41 - INFO - __main__ -     global_step = 7246
02/12/2023 03:04:41 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:04:41 - INFO - __main__ -     ********************
02/12/2023 03:04:44 - INFO - __main__ -   Epoch 62, the accuracy is 1.0
02/12/2023 03:05:12 - INFO - __main__ -   Epoch 63, step 99, train loss 0.0
02/12/2023 03:05:17 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:05:17 - INFO - __main__ -     Num examples = 89
02/12/2023 03:05:17 - INFO - __main__ -     Batch size = 8
02/12/2023 03:05:18 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:05:18 - INFO - __main__ -     global_step = 7361
02/12/2023 03:05:18 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:05:18 - INFO - __main__ -     ********************
02/12/2023 03:05:20 - INFO - __main__ -   Epoch 63, the accuracy is 1.0
02/12/2023 03:05:49 - INFO - __main__ -   Epoch 64, step 99, train loss 0.0
02/12/2023 03:05:53 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:05:53 - INFO - __main__ -     Num examples = 89
02/12/2023 03:05:53 - INFO - __main__ -     Batch size = 8
02/12/2023 03:05:54 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:05:54 - INFO - __main__ -     global_step = 7476
02/12/2023 03:05:54 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:05:54 - INFO - __main__ -     ********************
02/12/2023 03:05:57 - INFO - __main__ -   Epoch 64, the accuracy is 1.0
02/12/2023 03:06:26 - INFO - __main__ -   Epoch 65, step 99, train loss 0.0
02/12/2023 03:06:30 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:06:30 - INFO - __main__ -     Num examples = 89
02/12/2023 03:06:30 - INFO - __main__ -     Batch size = 8
02/12/2023 03:06:31 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:06:31 - INFO - __main__ -     global_step = 7591
02/12/2023 03:06:31 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:06:31 - INFO - __main__ -     ********************
02/12/2023 03:06:34 - INFO - __main__ -   Epoch 65, the accuracy is 1.0
02/12/2023 03:07:03 - INFO - __main__ -   Epoch 66, step 99, train loss 0.0
02/12/2023 03:07:07 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:07:07 - INFO - __main__ -     Num examples = 89
02/12/2023 03:07:07 - INFO - __main__ -     Batch size = 8
02/12/2023 03:07:08 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:07:08 - INFO - __main__ -     global_step = 7706
02/12/2023 03:07:08 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:07:08 - INFO - __main__ -     ********************
02/12/2023 03:07:09 - INFO - __main__ -     Best ppl:1.0
02/12/2023 03:07:09 - INFO - __main__ -     ********************
02/12/2023 03:07:12 - INFO - __main__ -   Epoch 66, the accuracy is 1.0
02/12/2023 03:07:41 - INFO - __main__ -   Epoch 67, step 99, train loss 0.0
02/12/2023 03:07:45 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:07:45 - INFO - __main__ -     Num examples = 89
02/12/2023 03:07:45 - INFO - __main__ -     Batch size = 8
02/12/2023 03:07:46 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:07:46 - INFO - __main__ -     global_step = 7821
02/12/2023 03:07:46 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:07:46 - INFO - __main__ -     ********************
02/12/2023 03:07:47 - INFO - __main__ -     Best ppl:1.0
02/12/2023 03:07:47 - INFO - __main__ -     ********************
02/12/2023 03:07:50 - INFO - __main__ -   Epoch 67, the accuracy is 1.0
02/12/2023 03:08:18 - INFO - __main__ -   Epoch 68, step 99, train loss 0.0
02/12/2023 03:08:23 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:08:23 - INFO - __main__ -     Num examples = 89
02/12/2023 03:08:23 - INFO - __main__ -     Batch size = 8
02/12/2023 03:08:24 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:08:24 - INFO - __main__ -     global_step = 7936
02/12/2023 03:08:24 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:08:24 - INFO - __main__ -     ********************
02/12/2023 03:08:25 - INFO - __main__ -     Best ppl:1.0
02/12/2023 03:08:25 - INFO - __main__ -     ********************
02/12/2023 03:08:28 - INFO - __main__ -   Epoch 68, the accuracy is 1.0
02/12/2023 03:08:56 - INFO - __main__ -   Epoch 69, step 99, train loss 0.0
02/12/2023 03:09:01 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:09:01 - INFO - __main__ -     Num examples = 89
02/12/2023 03:09:01 - INFO - __main__ -     Batch size = 8
02/12/2023 03:09:02 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:09:02 - INFO - __main__ -     global_step = 8051
02/12/2023 03:09:02 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:09:02 - INFO - __main__ -     ********************
02/12/2023 03:09:04 - INFO - __main__ -   Epoch 69, the accuracy is 1.0
02/12/2023 03:09:33 - INFO - __main__ -   Epoch 70, step 99, train loss 0.0
02/12/2023 03:09:37 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:09:37 - INFO - __main__ -     Num examples = 89
02/12/2023 03:09:37 - INFO - __main__ -     Batch size = 8
02/12/2023 03:09:38 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:09:38 - INFO - __main__ -     global_step = 8166
02/12/2023 03:09:38 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:09:38 - INFO - __main__ -     ********************
02/12/2023 03:09:41 - INFO - __main__ -   Epoch 70, the accuracy is 1.0
02/12/2023 03:10:09 - INFO - __main__ -   Epoch 71, step 99, train loss 0.0
02/12/2023 03:10:14 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:10:14 - INFO - __main__ -     Num examples = 89
02/12/2023 03:10:14 - INFO - __main__ -     Batch size = 8
02/12/2023 03:10:15 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:10:15 - INFO - __main__ -     global_step = 8281
02/12/2023 03:10:15 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:10:15 - INFO - __main__ -     ********************
02/12/2023 03:10:17 - INFO - __main__ -   Epoch 71, the accuracy is 1.0
02/12/2023 03:10:46 - INFO - __main__ -   Epoch 72, step 99, train loss 0.0
02/12/2023 03:10:51 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:10:51 - INFO - __main__ -     Num examples = 89
02/12/2023 03:10:51 - INFO - __main__ -     Batch size = 8
02/12/2023 03:10:52 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:10:52 - INFO - __main__ -     global_step = 8396
02/12/2023 03:10:52 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:10:52 - INFO - __main__ -     ********************
02/12/2023 03:10:53 - INFO - __main__ -     Best ppl:1.0
02/12/2023 03:10:53 - INFO - __main__ -     ********************
02/12/2023 03:10:56 - INFO - __main__ -   Epoch 72, the accuracy is 1.0
02/12/2023 03:11:24 - INFO - __main__ -   Epoch 73, step 99, train loss 0.0
02/12/2023 03:11:29 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:11:29 - INFO - __main__ -     Num examples = 89
02/12/2023 03:11:29 - INFO - __main__ -     Batch size = 8
02/12/2023 03:11:30 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:11:30 - INFO - __main__ -     global_step = 8511
02/12/2023 03:11:30 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:11:30 - INFO - __main__ -     ********************
02/12/2023 03:11:32 - INFO - __main__ -   Epoch 73, the accuracy is 1.0
02/12/2023 03:12:01 - INFO - __main__ -   Epoch 74, step 99, train loss 0.0
02/12/2023 03:12:05 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:12:05 - INFO - __main__ -     Num examples = 89
02/12/2023 03:12:05 - INFO - __main__ -     Batch size = 8
02/12/2023 03:12:06 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:12:06 - INFO - __main__ -     global_step = 8626
02/12/2023 03:12:06 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:12:06 - INFO - __main__ -     ********************
02/12/2023 03:12:08 - INFO - __main__ -   Epoch 74, the accuracy is 1.0
02/12/2023 03:12:37 - INFO - __main__ -   Epoch 75, step 99, train loss 0.0
02/12/2023 03:12:42 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:12:42 - INFO - __main__ -     Num examples = 89
02/12/2023 03:12:42 - INFO - __main__ -     Batch size = 8
02/12/2023 03:12:43 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:12:43 - INFO - __main__ -     global_step = 8741
02/12/2023 03:12:43 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:12:43 - INFO - __main__ -     ********************
02/12/2023 03:12:45 - INFO - __main__ -   Epoch 75, the accuracy is 1.0
02/12/2023 03:13:14 - INFO - __main__ -   Epoch 76, step 99, train loss 0.0
02/12/2023 03:13:18 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:13:18 - INFO - __main__ -     Num examples = 89
02/12/2023 03:13:18 - INFO - __main__ -     Batch size = 8
02/12/2023 03:13:19 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:13:19 - INFO - __main__ -     global_step = 8856
02/12/2023 03:13:19 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:13:19 - INFO - __main__ -     ********************
02/12/2023 03:13:21 - INFO - __main__ -   Epoch 76, the accuracy is 1.0
02/12/2023 03:13:50 - INFO - __main__ -   Epoch 77, step 99, train loss 0.0
02/12/2023 03:13:55 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:13:55 - INFO - __main__ -     Num examples = 89
02/12/2023 03:13:55 - INFO - __main__ -     Batch size = 8
02/12/2023 03:13:56 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:13:56 - INFO - __main__ -     global_step = 8971
02/12/2023 03:13:56 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:13:56 - INFO - __main__ -     ********************
02/12/2023 03:13:58 - INFO - __main__ -   Epoch 77, the accuracy is 1.0
02/12/2023 03:14:27 - INFO - __main__ -   Epoch 78, step 99, train loss 0.0
02/12/2023 03:14:31 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:14:31 - INFO - __main__ -     Num examples = 89
02/12/2023 03:14:31 - INFO - __main__ -     Batch size = 8
02/12/2023 03:14:32 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:14:32 - INFO - __main__ -     global_step = 9086
02/12/2023 03:14:32 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:14:32 - INFO - __main__ -     ********************
02/12/2023 03:14:34 - INFO - __main__ -   Epoch 78, the accuracy is 1.0
02/12/2023 03:15:03 - INFO - __main__ -   Epoch 79, step 99, train loss 0.0
02/12/2023 03:15:08 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:15:08 - INFO - __main__ -     Num examples = 89
02/12/2023 03:15:08 - INFO - __main__ -     Batch size = 8
02/12/2023 03:15:09 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:15:09 - INFO - __main__ -     global_step = 9201
02/12/2023 03:15:09 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:15:09 - INFO - __main__ -     ********************
02/12/2023 03:15:11 - INFO - __main__ -   Epoch 79, the accuracy is 1.0
02/12/2023 03:15:40 - INFO - __main__ -   Epoch 80, step 99, train loss 0.0
02/12/2023 03:15:44 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:15:44 - INFO - __main__ -     Num examples = 89
02/12/2023 03:15:44 - INFO - __main__ -     Batch size = 8
02/12/2023 03:15:45 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:15:45 - INFO - __main__ -     global_step = 9316
02/12/2023 03:15:45 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:15:45 - INFO - __main__ -     ********************
02/12/2023 03:15:47 - INFO - __main__ -   Epoch 80, the accuracy is 1.0
02/12/2023 03:16:16 - INFO - __main__ -   Epoch 81, step 99, train loss 0.0
02/12/2023 03:16:21 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:16:21 - INFO - __main__ -     Num examples = 89
02/12/2023 03:16:21 - INFO - __main__ -     Batch size = 8
02/12/2023 03:16:22 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:16:22 - INFO - __main__ -     global_step = 9431
02/12/2023 03:16:22 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:16:22 - INFO - __main__ -     ********************
02/12/2023 03:16:24 - INFO - __main__ -   Epoch 81, the accuracy is 1.0
02/12/2023 03:16:53 - INFO - __main__ -   Epoch 82, step 99, train loss 0.0
02/12/2023 03:16:57 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:16:57 - INFO - __main__ -     Num examples = 89
02/12/2023 03:16:57 - INFO - __main__ -     Batch size = 8
02/12/2023 03:16:58 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:16:58 - INFO - __main__ -     global_step = 9546
02/12/2023 03:16:58 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:16:58 - INFO - __main__ -     ********************
02/12/2023 03:17:01 - INFO - __main__ -   Epoch 82, the accuracy is 1.0
02/12/2023 03:17:29 - INFO - __main__ -   Epoch 83, step 99, train loss 0.0
02/12/2023 03:17:34 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:17:34 - INFO - __main__ -     Num examples = 89
02/12/2023 03:17:34 - INFO - __main__ -     Batch size = 8
02/12/2023 03:17:35 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:17:35 - INFO - __main__ -     global_step = 9661
02/12/2023 03:17:35 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:17:35 - INFO - __main__ -     ********************
02/12/2023 03:17:37 - INFO - __main__ -   Epoch 83, the accuracy is 1.0
02/12/2023 03:18:06 - INFO - __main__ -   Epoch 84, step 99, train loss 0.0
02/12/2023 03:18:10 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:18:10 - INFO - __main__ -     Num examples = 89
02/12/2023 03:18:10 - INFO - __main__ -     Batch size = 8
02/12/2023 03:18:11 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:18:11 - INFO - __main__ -     global_step = 9776
02/12/2023 03:18:11 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:18:11 - INFO - __main__ -     ********************
02/12/2023 03:18:14 - INFO - __main__ -   Epoch 84, the accuracy is 1.0
02/12/2023 03:18:42 - INFO - __main__ -   Epoch 85, step 99, train loss 0.0
02/12/2023 03:18:47 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:18:47 - INFO - __main__ -     Num examples = 89
02/12/2023 03:18:47 - INFO - __main__ -     Batch size = 8
02/12/2023 03:18:48 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:18:48 - INFO - __main__ -     global_step = 9891
02/12/2023 03:18:48 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:18:48 - INFO - __main__ -     ********************
02/12/2023 03:18:50 - INFO - __main__ -   Epoch 85, the accuracy is 1.0
02/12/2023 03:19:19 - INFO - __main__ -   Epoch 86, step 99, train loss 0.0
02/12/2023 03:19:23 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:19:23 - INFO - __main__ -     Num examples = 89
02/12/2023 03:19:23 - INFO - __main__ -     Batch size = 8
02/12/2023 03:19:24 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:19:24 - INFO - __main__ -     global_step = 10006
02/12/2023 03:19:24 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:19:24 - INFO - __main__ -     ********************
02/12/2023 03:19:27 - INFO - __main__ -   Epoch 86, the accuracy is 1.0
02/12/2023 03:19:55 - INFO - __main__ -   Epoch 87, step 99, train loss 0.0
02/12/2023 03:20:00 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:20:00 - INFO - __main__ -     Num examples = 89
02/12/2023 03:20:00 - INFO - __main__ -     Batch size = 8
02/12/2023 03:20:01 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:20:01 - INFO - __main__ -     global_step = 10121
02/12/2023 03:20:01 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:20:01 - INFO - __main__ -     ********************
02/12/2023 03:20:03 - INFO - __main__ -   Epoch 87, the accuracy is 1.0
02/12/2023 03:20:32 - INFO - __main__ -   Epoch 88, step 99, train loss 0.0
02/12/2023 03:20:37 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:20:37 - INFO - __main__ -     Num examples = 89
02/12/2023 03:20:37 - INFO - __main__ -     Batch size = 8
02/12/2023 03:20:38 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:20:38 - INFO - __main__ -     global_step = 10236
02/12/2023 03:20:38 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:20:38 - INFO - __main__ -     ********************
02/12/2023 03:20:40 - INFO - __main__ -   Epoch 88, the accuracy is 1.0
02/12/2023 03:21:09 - INFO - __main__ -   Epoch 89, step 99, train loss 0.0
02/12/2023 03:21:13 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:21:13 - INFO - __main__ -     Num examples = 89
02/12/2023 03:21:13 - INFO - __main__ -     Batch size = 8
02/12/2023 03:21:14 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:21:14 - INFO - __main__ -     global_step = 10351
02/12/2023 03:21:14 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:21:14 - INFO - __main__ -     ********************
02/12/2023 03:21:17 - INFO - __main__ -   Epoch 89, the accuracy is 1.0
02/12/2023 03:21:46 - INFO - __main__ -   Epoch 90, step 99, train loss 0.0
02/12/2023 03:21:50 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:21:50 - INFO - __main__ -     Num examples = 89
02/12/2023 03:21:50 - INFO - __main__ -     Batch size = 8
02/12/2023 03:21:51 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:21:51 - INFO - __main__ -     global_step = 10466
02/12/2023 03:21:51 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:21:51 - INFO - __main__ -     ********************
02/12/2023 03:21:54 - INFO - __main__ -   Epoch 90, the accuracy is 1.0
02/12/2023 03:22:23 - INFO - __main__ -   Epoch 91, step 99, train loss 0.0
02/12/2023 03:22:27 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:22:27 - INFO - __main__ -     Num examples = 89
02/12/2023 03:22:27 - INFO - __main__ -     Batch size = 8
02/12/2023 03:22:28 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:22:28 - INFO - __main__ -     global_step = 10581
02/12/2023 03:22:28 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:22:28 - INFO - __main__ -     ********************
02/12/2023 03:22:31 - INFO - __main__ -   Epoch 91, the accuracy is 1.0
02/12/2023 03:23:00 - INFO - __main__ -   Epoch 92, step 99, train loss 0.0
02/12/2023 03:23:04 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:23:04 - INFO - __main__ -     Num examples = 89
02/12/2023 03:23:04 - INFO - __main__ -     Batch size = 8
02/12/2023 03:23:05 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:23:05 - INFO - __main__ -     global_step = 10696
02/12/2023 03:23:05 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:23:05 - INFO - __main__ -     ********************
02/12/2023 03:23:07 - INFO - __main__ -   Epoch 92, the accuracy is 1.0
02/12/2023 03:23:36 - INFO - __main__ -   Epoch 93, step 99, train loss 0.0
02/12/2023 03:23:41 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:23:41 - INFO - __main__ -     Num examples = 89
02/12/2023 03:23:41 - INFO - __main__ -     Batch size = 8
02/12/2023 03:23:42 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:23:42 - INFO - __main__ -     global_step = 10811
02/12/2023 03:23:42 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:23:42 - INFO - __main__ -     ********************
02/12/2023 03:23:44 - INFO - __main__ -   Epoch 93, the accuracy is 1.0
02/12/2023 03:24:13 - INFO - __main__ -   Epoch 94, step 99, train loss 0.0
02/12/2023 03:24:17 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:24:17 - INFO - __main__ -     Num examples = 89
02/12/2023 03:24:17 - INFO - __main__ -     Batch size = 8
02/12/2023 03:24:18 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:24:18 - INFO - __main__ -     global_step = 10926
02/12/2023 03:24:18 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:24:18 - INFO - __main__ -     ********************
02/12/2023 03:24:20 - INFO - __main__ -   Epoch 94, the accuracy is 1.0
02/12/2023 03:24:49 - INFO - __main__ -   Epoch 95, step 99, train loss 0.0
02/12/2023 03:24:54 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:24:54 - INFO - __main__ -     Num examples = 89
02/12/2023 03:24:54 - INFO - __main__ -     Batch size = 8
02/12/2023 03:24:55 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:24:55 - INFO - __main__ -     global_step = 11041
02/12/2023 03:24:55 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:24:55 - INFO - __main__ -     ********************
02/12/2023 03:24:57 - INFO - __main__ -   Epoch 95, the accuracy is 1.0
02/12/2023 03:25:26 - INFO - __main__ -   Epoch 96, step 99, train loss 0.0
02/12/2023 03:25:30 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:25:30 - INFO - __main__ -     Num examples = 89
02/12/2023 03:25:30 - INFO - __main__ -     Batch size = 8
02/12/2023 03:25:31 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:25:31 - INFO - __main__ -     global_step = 11156
02/12/2023 03:25:31 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:25:31 - INFO - __main__ -     ********************
02/12/2023 03:25:33 - INFO - __main__ -   Epoch 96, the accuracy is 1.0
02/12/2023 03:26:02 - INFO - __main__ -   Epoch 97, step 99, train loss 0.0
02/12/2023 03:26:07 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:26:07 - INFO - __main__ -     Num examples = 89
02/12/2023 03:26:07 - INFO - __main__ -     Batch size = 8
02/12/2023 03:26:08 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:26:08 - INFO - __main__ -     global_step = 11271
02/12/2023 03:26:08 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:26:08 - INFO - __main__ -     ********************
02/12/2023 03:26:10 - INFO - __main__ -   Epoch 97, the accuracy is 1.0
02/12/2023 03:26:39 - INFO - __main__ -   Epoch 98, step 99, train loss 0.0
02/12/2023 03:26:43 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:26:43 - INFO - __main__ -     Num examples = 89
02/12/2023 03:26:43 - INFO - __main__ -     Batch size = 8
02/12/2023 03:26:44 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:26:44 - INFO - __main__ -     global_step = 11386
02/12/2023 03:26:44 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:26:44 - INFO - __main__ -     ********************
02/12/2023 03:26:47 - INFO - __main__ -   Epoch 98, the accuracy is 1.0
02/12/2023 03:27:15 - INFO - __main__ -   Epoch 99, step 99, train loss 0.0
02/12/2023 03:27:20 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:27:20 - INFO - __main__ -     Num examples = 89
02/12/2023 03:27:20 - INFO - __main__ -     Batch size = 8
02/12/2023 03:27:21 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:27:21 - INFO - __main__ -     global_step = 11501
02/12/2023 03:27:21 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:27:21 - INFO - __main__ -     ********************
02/12/2023 03:27:23 - INFO - __main__ -   Epoch 99, the accuracy is 1.0
02/12/2023 03:27:52 - INFO - __main__ -   Epoch 100, step 99, train loss 0.0
02/12/2023 03:27:56 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:27:56 - INFO - __main__ -     Num examples = 89
02/12/2023 03:27:56 - INFO - __main__ -     Batch size = 8
02/12/2023 03:27:57 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:27:57 - INFO - __main__ -     global_step = 11616
02/12/2023 03:27:57 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:27:57 - INFO - __main__ -     ********************
02/12/2023 03:28:00 - INFO - __main__ -   Epoch 100, the accuracy is 1.0
02/12/2023 03:28:28 - INFO - __main__ -   Epoch 101, step 99, train loss 0.0
02/12/2023 03:28:33 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:28:33 - INFO - __main__ -     Num examples = 89
02/12/2023 03:28:33 - INFO - __main__ -     Batch size = 8
02/12/2023 03:28:34 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:28:34 - INFO - __main__ -     global_step = 11731
02/12/2023 03:28:34 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:28:34 - INFO - __main__ -     ********************
02/12/2023 03:28:36 - INFO - __main__ -   Epoch 101, the accuracy is 1.0
02/12/2023 03:29:05 - INFO - __main__ -   Epoch 102, step 99, train loss 0.0
02/12/2023 03:29:10 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:29:10 - INFO - __main__ -     Num examples = 89
02/12/2023 03:29:10 - INFO - __main__ -     Batch size = 8
02/12/2023 03:29:10 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:29:10 - INFO - __main__ -     global_step = 11846
02/12/2023 03:29:10 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:29:10 - INFO - __main__ -     ********************
02/12/2023 03:29:13 - INFO - __main__ -   Epoch 102, the accuracy is 1.0
02/12/2023 03:29:41 - INFO - __main__ -   Epoch 103, step 99, train loss 0.0
02/12/2023 03:29:46 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:29:46 - INFO - __main__ -     Num examples = 89
02/12/2023 03:29:46 - INFO - __main__ -     Batch size = 8
02/12/2023 03:29:47 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:29:47 - INFO - __main__ -     global_step = 11961
02/12/2023 03:29:47 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:29:47 - INFO - __main__ -     ********************
02/12/2023 03:29:49 - INFO - __main__ -   Epoch 103, the accuracy is 1.0
02/12/2023 03:30:18 - INFO - __main__ -   Epoch 104, step 99, train loss 0.0
02/12/2023 03:30:22 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:30:22 - INFO - __main__ -     Num examples = 89
02/12/2023 03:30:22 - INFO - __main__ -     Batch size = 8
02/12/2023 03:30:23 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:30:23 - INFO - __main__ -     global_step = 12076
02/12/2023 03:30:23 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:30:23 - INFO - __main__ -     ********************
02/12/2023 03:30:26 - INFO - __main__ -   Epoch 104, the accuracy is 1.0
02/12/2023 03:30:55 - INFO - __main__ -   Epoch 105, step 99, train loss 0.0
02/12/2023 03:30:59 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:30:59 - INFO - __main__ -     Num examples = 89
02/12/2023 03:30:59 - INFO - __main__ -     Batch size = 8
02/12/2023 03:31:00 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:31:00 - INFO - __main__ -     global_step = 12191
02/12/2023 03:31:00 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:31:00 - INFO - __main__ -     ********************
02/12/2023 03:31:02 - INFO - __main__ -   Epoch 105, the accuracy is 1.0
02/12/2023 03:31:31 - INFO - __main__ -   Epoch 106, step 99, train loss 0.0
02/12/2023 03:31:36 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:31:36 - INFO - __main__ -     Num examples = 89
02/12/2023 03:31:36 - INFO - __main__ -     Batch size = 8
02/12/2023 03:31:36 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:31:36 - INFO - __main__ -     global_step = 12306
02/12/2023 03:31:36 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:31:36 - INFO - __main__ -     ********************
02/12/2023 03:31:39 - INFO - __main__ -   Epoch 106, the accuracy is 1.0
02/12/2023 03:32:07 - INFO - __main__ -   Epoch 107, step 99, train loss 0.0
02/12/2023 03:32:12 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:32:12 - INFO - __main__ -     Num examples = 89
02/12/2023 03:32:12 - INFO - __main__ -     Batch size = 8
02/12/2023 03:32:13 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:32:13 - INFO - __main__ -     global_step = 12421
02/12/2023 03:32:13 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:32:13 - INFO - __main__ -     ********************
02/12/2023 03:32:15 - INFO - __main__ -   Epoch 107, the accuracy is 1.0
02/12/2023 03:32:44 - INFO - __main__ -   Epoch 108, step 99, train loss 0.0
02/12/2023 03:32:49 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:32:49 - INFO - __main__ -     Num examples = 89
02/12/2023 03:32:49 - INFO - __main__ -     Batch size = 8
02/12/2023 03:32:50 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:32:50 - INFO - __main__ -     global_step = 12536
02/12/2023 03:32:50 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:32:50 - INFO - __main__ -     ********************
02/12/2023 03:32:52 - INFO - __main__ -   Epoch 108, the accuracy is 1.0
02/12/2023 03:33:21 - INFO - __main__ -   Epoch 109, step 99, train loss 0.0
02/12/2023 03:33:25 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:33:25 - INFO - __main__ -     Num examples = 89
02/12/2023 03:33:25 - INFO - __main__ -     Batch size = 8
02/12/2023 03:33:26 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:33:26 - INFO - __main__ -     global_step = 12651
02/12/2023 03:33:26 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:33:26 - INFO - __main__ -     ********************
02/12/2023 03:33:29 - INFO - __main__ -   Epoch 109, the accuracy is 1.0
02/12/2023 03:33:57 - INFO - __main__ -   Epoch 110, step 99, train loss 0.0
02/12/2023 03:34:02 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:34:02 - INFO - __main__ -     Num examples = 89
02/12/2023 03:34:02 - INFO - __main__ -     Batch size = 8
02/12/2023 03:34:03 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:34:03 - INFO - __main__ -     global_step = 12766
02/12/2023 03:34:03 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:34:03 - INFO - __main__ -     ********************
02/12/2023 03:34:05 - INFO - __main__ -   Epoch 110, the accuracy is 1.0
02/12/2023 03:34:34 - INFO - __main__ -   Epoch 111, step 99, train loss 0.0
02/12/2023 03:34:39 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:34:39 - INFO - __main__ -     Num examples = 89
02/12/2023 03:34:39 - INFO - __main__ -     Batch size = 8
02/12/2023 03:34:40 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:34:40 - INFO - __main__ -     global_step = 12881
02/12/2023 03:34:40 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:34:40 - INFO - __main__ -     ********************
02/12/2023 03:34:41 - INFO - __main__ -     Best ppl:1.0
02/12/2023 03:34:41 - INFO - __main__ -     ********************
02/12/2023 03:34:44 - INFO - __main__ -   Epoch 111, the accuracy is 1.0
02/12/2023 03:35:13 - INFO - __main__ -   Epoch 112, step 99, train loss 0.0
02/12/2023 03:35:17 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:35:17 - INFO - __main__ -     Num examples = 89
02/12/2023 03:35:17 - INFO - __main__ -     Batch size = 8
02/12/2023 03:35:18 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:35:18 - INFO - __main__ -     global_step = 12996
02/12/2023 03:35:18 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:35:18 - INFO - __main__ -     ********************
02/12/2023 03:35:20 - INFO - __main__ -   Epoch 112, the accuracy is 1.0
02/12/2023 03:35:49 - INFO - __main__ -   Epoch 113, step 99, train loss 0.0
02/12/2023 03:35:54 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:35:54 - INFO - __main__ -     Num examples = 89
02/12/2023 03:35:54 - INFO - __main__ -     Batch size = 8
02/12/2023 03:35:54 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:35:54 - INFO - __main__ -     global_step = 13111
02/12/2023 03:35:54 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:35:54 - INFO - __main__ -     ********************
02/12/2023 03:35:56 - INFO - __main__ -     Best ppl:1.0
02/12/2023 03:35:56 - INFO - __main__ -     ********************
02/12/2023 03:35:59 - INFO - __main__ -   Epoch 113, the accuracy is 1.0
02/12/2023 03:36:27 - INFO - __main__ -   Epoch 114, step 99, train loss 0.0
02/12/2023 03:36:32 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:36:32 - INFO - __main__ -     Num examples = 89
02/12/2023 03:36:32 - INFO - __main__ -     Batch size = 8
02/12/2023 03:36:33 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:36:33 - INFO - __main__ -     global_step = 13226
02/12/2023 03:36:33 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:36:33 - INFO - __main__ -     ********************
02/12/2023 03:36:35 - INFO - __main__ -   Epoch 114, the accuracy is 1.0
02/12/2023 03:37:04 - INFO - __main__ -   Epoch 115, step 99, train loss 0.0
02/12/2023 03:37:09 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:37:09 - INFO - __main__ -     Num examples = 89
02/12/2023 03:37:09 - INFO - __main__ -     Batch size = 8
02/12/2023 03:37:09 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:37:09 - INFO - __main__ -     global_step = 13341
02/12/2023 03:37:09 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:37:09 - INFO - __main__ -     ********************
02/12/2023 03:37:11 - INFO - __main__ -     Best ppl:1.0
02/12/2023 03:37:11 - INFO - __main__ -     ********************
02/12/2023 03:37:13 - INFO - __main__ -   Epoch 115, the accuracy is 1.0
02/12/2023 03:37:42 - INFO - __main__ -   Epoch 116, step 99, train loss 0.0
02/12/2023 03:37:47 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:37:47 - INFO - __main__ -     Num examples = 89
02/12/2023 03:37:47 - INFO - __main__ -     Batch size = 8
02/12/2023 03:37:48 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:37:48 - INFO - __main__ -     global_step = 13456
02/12/2023 03:37:48 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:37:48 - INFO - __main__ -     ********************
02/12/2023 03:37:50 - INFO - __main__ -   Epoch 116, the accuracy is 1.0
02/12/2023 03:38:19 - INFO - __main__ -   Epoch 117, step 99, train loss 0.0
02/12/2023 03:38:24 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:38:24 - INFO - __main__ -     Num examples = 89
02/12/2023 03:38:24 - INFO - __main__ -     Batch size = 8
02/12/2023 03:38:25 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:38:25 - INFO - __main__ -     global_step = 13571
02/12/2023 03:38:25 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:38:25 - INFO - __main__ -     ********************
02/12/2023 03:38:27 - INFO - __main__ -   Epoch 117, the accuracy is 1.0
02/12/2023 03:38:56 - INFO - __main__ -   Epoch 118, step 99, train loss 0.0
02/12/2023 03:39:00 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:39:00 - INFO - __main__ -     Num examples = 89
02/12/2023 03:39:00 - INFO - __main__ -     Batch size = 8
02/12/2023 03:39:01 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:39:01 - INFO - __main__ -     global_step = 13686
02/12/2023 03:39:01 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:39:01 - INFO - __main__ -     ********************
02/12/2023 03:39:04 - INFO - __main__ -   Epoch 118, the accuracy is 1.0
02/12/2023 03:39:33 - INFO - __main__ -   Epoch 119, step 99, train loss 0.0
02/12/2023 03:39:37 - INFO - __main__ -   
***** Running evaluation *****
02/12/2023 03:39:37 - INFO - __main__ -     Num examples = 89
02/12/2023 03:39:37 - INFO - __main__ -     Batch size = 8
02/12/2023 03:39:38 - INFO - __main__ -     eval_ppl = 1.0
02/12/2023 03:39:38 - INFO - __main__ -     global_step = 13801
02/12/2023 03:39:38 - INFO - __main__ -     train_loss = 0.0
02/12/2023 03:39:38 - INFO - __main__ -     ********************
02/12/2023 03:39:40 - INFO - __main__ -   Epoch 119, the accuracy is 1.0
02/12/2023 03:39:40 - INFO - __main__ -   Test file: final_final_dataset/java/val_data_of_Ownership.jsonl
02/12/2023 03:39:41 - INFO - __main__ -   gold_info:{'all_count': 89, 'Positive': 4, 'Negative': 85}
02/12/2023 03:39:41 - INFO - __main__ -   pre_info:{'TP': 4, 'FP': 0, 'TN': 85, 'FN': 0}
02/12/2023 03:39:41 - INFO - __main__ -   Epoch 119, the accuracy is 1.0, the precision is 1.0, the recall is 1.0, the fscore is 1.0
02/12/2023 03:39:41 - INFO - __main__ -   Test file: final_final_dataset/java/test_data_of_Ownership.jsonl
02/12/2023 03:39:47 - INFO - __main__ -   gold_info:{'all_count': 489, 'Positive': 25, 'Negative': 464}
02/12/2023 03:39:47 - INFO - __main__ -   pre_info:{'TP': 25, 'FP': 0, 'TN': 464, 'FN': 0}
02/12/2023 03:39:47 - INFO - __main__ -   Epoch 119, the accuracy is 1.0, the precision is 1.0, the recall is 1.0, the fscore is 1.0
 03:39:41 - INFO - __main__ -   Epoch 119, the accuracy is 1.0, the precision is 1.0, the recall is 1.0, the fscore is 1.0
02/12/2023 03:39:41 - INFO - __main__ -   Test file: final_final_dataset/java/test_data_of_Ownership.jsonl
02/12/2023 03:39:47 - INFO - __main__ -   gold_info:{'all_count': 489, 'Positive': 25, 'Negative': 464}
02/12/2023 03:39:47 - INFO - __main__ -   pre_info:{'TP': 25, 'FP': 0, 'TN': 464, 'FN': 0}
02/12/2023 03:39:47 - INFO - __main__ -   Epoch 119, the accuracy is 1.0, the precision is 1.0, the recall is 1.0, the fscore is 1.0
